{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da36c36022d6181",
   "metadata": {},
   "source": [
    "In this tutorial we will use the realsense camera mounted on the robot.\n",
    "Note that the camera interface and hardware is independent of the robots interface and hardware.\n",
    "In order to use the camera you just need to connect it's USB to your machine. If you get errors, you may need to install the realsense drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ec47a380acf314",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:55:22.295934Z",
     "start_time": "2025-03-04T09:55:22.061747Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e8ced0b11fec3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We connect here to the robot since we are going to move it during this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914703de70ce0e99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:56:01.618121Z",
     "start_time": "2025-03-04T09:55:22.528717Z"
    }
   },
   "outputs": [],
   "source": [
    "from clair_robotics_stack.ur.lab_setup.manipulation.manipulation_controller_2fg import ManipulationController2FG\n",
    "from clair_robotics_stack.ur.lab_setup.robot_inteface.robots_metadata import ur5e_1\n",
    "\n",
    "robot = ManipulationController2FG.build_from_robot_name_and_ip(ur5e_1[\"ip\"], ur5e_1[\"name\"], visualize=True)\n",
    "robot.plan_and_move_home()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab80f2e381f19e3b",
   "metadata": {},
   "source": [
    "# Realsense Camera Interface\n",
    "The camera interface is very simple. You create a camera object and whenever you want, you call it to get a frame. It will return a frame for both color image and depth image. Note that there is a get_frame_bgr method as well, as cv2 uses BGR format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb991d1bb6aa055f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T08:25:52.015507Z",
     "start_time": "2025-03-04T08:25:51.877642Z"
    }
   },
   "outputs": [],
   "source": [
    "from clair_robotics_stack.camera.realsense_camera import RealsenseCamera\n",
    "\n",
    "camera = RealsenseCamera()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7303585b79fc86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T08:25:52.767494Z",
     "start_time": "2025-03-04T08:25:52.270363Z"
    }
   },
   "outputs": [],
   "source": [
    "# get a frame, returns both rgb and depth\n",
    "rgb, depth = camera.get_frame_rgb()\n",
    "\n",
    "# visualize rgb:\n",
    "plt.imshow(rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7592bf0a9d4a2a26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T08:25:53.300276Z",
     "start_time": "2025-03-04T08:25:53.128133Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize depth:\n",
    "plt.imshow(depth, cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14a4876c2c05f2d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As you can see, the depth image returns the distance in meters for each pixel. Note that it's noisy. Typically, you are probably not going to use it for scenes of more than 1-2 meters. Let's move the robot and take picture of the table and the surroundings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9381b9a1fdb962fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T08:25:57.502776Z",
     "start_time": "2025-03-04T08:25:54.765898Z"
    }
   },
   "outputs": [],
   "source": [
    "robot.plan_and_moveJ([-np.pi/4, -np.pi/2, -np.pi/2, -np.pi/2, np.pi/2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72d1657e4af8103",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T08:26:06.300216Z",
     "start_time": "2025-03-04T08:26:06.032971Z"
    }
   },
   "outputs": [],
   "source": [
    "rgb, depth = camera.get_frame_rgb()\n",
    "plt.imshow(rgb)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(depth, cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72796967e1e1b548",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Small noisy areas have pixels that shows too far distance and increase the scale. Let's clamp the depth to 2 meters to see better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9d6a618c7a351a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T08:26:12.707224Z",
     "start_time": "2025-03-04T08:26:12.531506Z"
    }
   },
   "outputs": [],
   "source": [
    "depth_clamped = np.clip(depth, 0, 2.0)\n",
    "plt.imshow(depth_clamped, cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292bb8604cc77e32",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here is a simple example how you can use cv2 to continuously stream the camera frames. You can use this to stream the camera while moving the robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f7dafb62ca745e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T08:26:23.313999Z",
     "start_time": "2025-03-04T08:26:14.651383Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# stream 200 frames, depth and rgb at about 30 fps while moving the robot:\n",
    "q0 = robot.getActualQ()\n",
    "q1 = q0.copy()\n",
    "q1[0] = np.pi/8\n",
    "# move between q1 and q0 asynchronously:\n",
    "robot.move_path([q1, q0], speed=.5, acceleration=1, asynchronous=True)\n",
    "\n",
    "# stream while moving:\n",
    "for i in range(200):\n",
    "    rgb, depth = camera.get_frame_rgb()\n",
    "    depth_clamped = np.clip(depth, 0, 3.0)\n",
    "    cv2.imshow('rgb', rgb)\n",
    "    cv2.imshow('depth', depth_clamped)\n",
    "    cv2.waitKey(33)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1e0998db834b93",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Stereo Camera\n",
    "The depth image from the realsense is produced through [stereo camera](https://en.wikipedia.org/wiki/Stereo_camera). The camera has two lenses and the depth is calculated by comparing the two images. The higher the offset of a point between the two cameras, the closer it is.\n",
    "This has two main implications on the quality of our depth images:\n",
    "1. They are noisy\n",
    "2. The depth around the edges of nearby objects can't be calculated, since those points are visible to only one camera at a time. Note the black areas around objects in the depth image. When a point is not seen by both cameras, it's depth is set to zero.\n",
    "3. The depth can't be computed for too nearby objects. In fact, the realsense camera (D415) can't \"see\" depth of more than 40cm. You have to consider that in your experiments\n",
    "\n",
    "Let's demonstrate the third point. We stream the depth image while getting closer to the table, not how when we get too close, we see noise instead of the actual table depth (while the floor is still visible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6198fb4121601e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T08:26:58.930514Z",
     "start_time": "2025-03-04T08:26:49.381841Z"
    }
   },
   "outputs": [],
   "source": [
    "robot.plan_and_moveJ(q0)\n",
    "\n",
    "# move down toward the table while streaming depth:\n",
    "robot.moveL_relative([0, 0, -0.2, 0, 0, 0], speed=0.05, acceleration=0.2, asynchronous=True)\n",
    "for i in range(100):\n",
    "    _, depth = camera.get_frame_rgb()\n",
    "    depth_clamped = np.clip(depth, 0, 3.0)\n",
    "    cv2.imshow('depth', depth_clamped)\n",
    "    cv2.waitKey(33)\n",
    "time.sleep(1)\n",
    "\n",
    "# move back up\n",
    "robot.moveL_relative([0, 0, 0.2, 0, 0, 0], speed=0.05, acceleration=0.2, asynchronous=True)\n",
    "for i in range(100):\n",
    "    _, depth = camera.get_frame_rgb()\n",
    "    depth_clamped = np.clip(depth, 0, 3.0)\n",
    "    cv2.imshow('depth', depth_clamped)\n",
    "    cv2.waitKey(33)\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12383d4e56261778",
   "metadata": {},
   "source": [
    "# Camera With Recording\n",
    "\n",
    "In addition to getting frames for robot vision, the camera can also be useful to record an experiment for debugging or for offline processing of videos. There is a camera interface that supports that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b54630ef895322a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T08:27:32.729440Z",
     "start_time": "2025-03-04T08:27:31.962801Z"
    }
   },
   "outputs": [],
   "source": [
    "del camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0de117bfce200",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:56:18.913672Z",
     "start_time": "2025-03-04T09:56:18.774243Z"
    }
   },
   "outputs": [],
   "source": [
    "from clair_robotics_stack.camera.realsense_camera import RealsenseCameraWithRecording\n",
    "\n",
    "camera_rec = RealsenseCameraWithRecording()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ea9b31fd93e16",
   "metadata": {},
   "source": [
    "To start recording, we just call the appropriate method and pass a directory for where the video will be saved. It is a directory and not a file name becasue it records two videos: RGB and depth. Recordings are saved in the path you provided as two files with the prefixes: _color.mp and _depth.mp4. The recording is saved when stop_recording() is called.\n",
    "\n",
    "There are other arguments you can pass to start recording:\n",
    "- Desired FPS (default is 30)\n",
    "- max depth distance (default is 5 meters). Depth in the recording will be claped to that value.\n",
    "\n",
    "Note that the start_recording() is non blocking - the recording logic is ran on a another process, possibly parallel to your experiment. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4623148b74e8b66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T08:59:30.389394Z",
     "start_time": "2025-03-04T08:59:24.902144Z"
    }
   },
   "outputs": [],
   "source": [
    "camera_rec.start_recording(file_path=\"test_recording/test_vid\")\n",
    "\n",
    "# can still get frames while recording:\n",
    "rgb, depth = camera_rec.get_frame_rgb()\n",
    "\n",
    "robot.plan_and_move_home()\n",
    "robot.plan_and_moveJ(q0)\n",
    "\n",
    "# don't forget to stop recording for the file to be saved:\n",
    "camera_rec.stop_recording()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef455c9789fcf62b",
   "metadata": {},
   "source": [
    "You can view the recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e82e226edc848ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T09:06:29.868950Z",
     "start_time": "2025-03-04T09:06:29.672879Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls test_recording"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7125b63888ca894f",
   "metadata": {},
   "source": [
    "For debugging, it is recommended to record the frames and perform your experiment inside a try block, and add finally block to stop the recording in case of failure. This way you will have the video even if there is a failure in your experiment.\n",
    "\n",
    "```python\n",
    "try:\n",
    "    camera_rec.start_recording(file_name=\"test_recording.mp4\")\n",
    "\n",
    "    # your code here\n",
    "\n",
    "finally:\n",
    "    camera_rec.stop_recording()\n",
    "    # your video is saved for debugging\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855278be5d4773a7",
   "metadata": {},
   "source": [
    "# Vision\n",
    "\n",
    "There are currently only basic vision modules which are practically examples of what you can do.\n",
    "We will use one of them here for position estimation of objects.\n",
    "\n",
    "**For preparation, put one of the wooden cubes on the table so it will be visible to the camera at q0.\n",
    "make sure it's edges is parallel to the table edges (it's not rotated).**\n",
    "\n",
    "We will use our Object Detection module. It is pretty simple, given an image, it provides a bounding box of an object. This is based on [YOLO-World](https://docs.ultralytics.com/models/yolo-world/) Model (Neural Network) which is very flexible: it can detect any object you specify via prompt (other versions of YOLO have limited number of classes of objects it can detect).\n",
    "Here we create an instance of our object detector that can detect \"wooden cube\", \"wooden block\" and \"wooden box\". It will return a detection if it's confidence that it's a true detection is above 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4809d83f154d67ff",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from clair_robotics_stack.vision.object_detection import ObjectDetection\n",
    "\n",
    "classes = ['wooden cube', 'wooden block', 'wooden box']\n",
    "detector = ObjectDetection(classes=classes, min_confidence=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6d5b1da05ca9e5",
   "metadata": {},
   "source": [
    "Now Let's move the robot and capture an image of the object from the q0 configuration. We need to remember what configuration the robot was in when we took the image, since we will need it to localize the object in the world later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7200461f9bb4f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T11:29:03.691231Z",
     "start_time": "2025-03-04T11:29:03.332492Z"
    }
   },
   "outputs": [],
   "source": [
    "q0 = [0, -np.pi/2, -np.pi/2, -np.pi/2, np.pi/2, 0]\n",
    "robot.plan_and_moveJ(q0)\n",
    "im, depth_im = camera_rec.get_frame_rgb()\n",
    "\n",
    "# show image:\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "depth_im = np.clip(depth_im, 0, 1.0)\n",
    "plt.imshow(depth_im, cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3e0a97ff1405e1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's pass the rgb image through the neural network and let it detect objects. The Neural networks accepts images in batches. We have only one image, so we will provide a batch of 1 image.\n",
    "\n",
    "The detector returns\n",
    "- coordinates of the bounding boxes for each image in the batch in the xyxy format (top left x, top left y, bottom right x, bottom right y)\n",
    "- confidences corresponding to each detection in each image\n",
    "- results object for each image that contains additional data including the original image\n",
    "\n",
    "Note that we pass a batch, and get results for a batch. In our case, we have only one image in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20addf1f37149f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T11:29:05.864780Z",
     "start_time": "2025-03-04T11:29:05.393622Z"
    }
   },
   "outputs": [],
   "source": [
    "# a batch is expected. provide batch of 1 image\n",
    "im_batch = [im]\n",
    "bboxes, confidences, results = detector.detect_objects(im_batch)\n",
    "\n",
    "#result is also returned as batch, we have only 1 element in the batch\n",
    "bboxes, confidences, results = bboxes[0], confidences[0], results[0]\n",
    "\n",
    "print()\n",
    "print(\"----\")\n",
    "print(\"bounding boxes xyxy and confidences: \")\n",
    "print(list(zip(bboxes, confidences)))\n",
    "\n",
    "# results contains additional data. we can use it to easily plot:\n",
    "im_annotated = detector.get_annotated_images(results)\n",
    "plt.imshow(im_annotated)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b433f98b843d0",
   "metadata": {},
   "source": [
    "### Finding the object in the world coordinates\n",
    "This will be practice for some of the tools we already know (and for some basics of computer vision geometry).\n",
    "Whatever you don't understand, please refer to online resources about camera geometry.\n",
    "\n",
    "First let's process to find the center of the cube in the image and the distance from the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410757b9a3e77ebf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T11:29:10.084877Z",
     "start_time": "2025-03-04T11:29:10.077962Z"
    }
   },
   "outputs": [],
   "source": [
    "# take the first detection:\n",
    "bbox = bboxes[0].cpu().numpy()\n",
    "center = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n",
    "center_int = [int(center[0]), int(center[1])]\n",
    "\n",
    "# get the depth at the center of the bounding, average on a small area around the center:\n",
    "window_in_depth = depth_im[center_int[1]-2:center_int[1]+2, center_int[0]-2:center_int[0]+2]\n",
    "depth_pixels_in_window = window_in_depth.flatten()\n",
    "# if some depth is zero, this is not a valid pixel:\n",
    "window_in_depth = window_in_depth[window_in_depth > 0]\n",
    "depth = np.mean(window_in_depth)\n",
    "\n",
    "print(\"center of object in image: \", center_int)\n",
    "print(\"depth at center: \", depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b30e7dd5b5bb48",
   "metadata": {},
   "source": [
    "\n",
    "Now let's convert it to 3D coordinates in the camera coordinate system.\n",
    "In camera coordinate system, Z is forward, Y is up, X is right. For this conversion we need to know the intrinsic parameters of the camera. We can retrieve them from the camera API. We use standard forumlas for the conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a47c76d18ca4c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T11:29:18.642596Z",
     "start_time": "2025-03-04T11:29:18.639481Z"
    }
   },
   "outputs": [],
   "source": [
    "from clair_robotics_stack.camera.configurations_and_params import color_camera_intrinsic_matrix\n",
    "\n",
    "fx = color_camera_intrinsic_matrix[0, 0]\n",
    "fy = color_camera_intrinsic_matrix[1, 1]\n",
    "ppx = color_camera_intrinsic_matrix[0, 2]\n",
    "ppy = color_camera_intrinsic_matrix[1, 2]\n",
    "\n",
    "x_cam = (center[0] - ppx) * depth / fx\n",
    "y_cam = (center[1] - ppy) * depth / fy\n",
    "z_cam = depth\n",
    "p_cam = [x_cam, y_cam, z_cam]\n",
    "\n",
    "print(\"object center in camera coordinates: \", p_cam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce456940ba7ec84c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we have the object center in the camera coordinates. We need to convert it to the world coordinates, but to manipulate the object we need its coordinate in the world frame of reference. We can do that by using the robot's current configuration (q0) and the robot's ground truth position in the world coordinates. We will use the geometry and transform module that was introduced in previous tutorials. Note that this conversion depends on the configuration of the robot when the image was taken because that determines the position of the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a296e0d0b59f0743",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T11:29:22.681145Z",
     "start_time": "2025-03-04T11:29:22.678278Z"
    }
   },
   "outputs": [],
   "source": [
    "gt = robot.gt\n",
    "point_world = gt.point_camera_to_world(p_cam, \"ur5e_1\", q0)\n",
    "\n",
    "print(\"point in world coordinates: \", point_world)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3df0a89c19bbbfe",
   "metadata": {},
   "source": [
    "Does that make sense given the actual position? Recall how the world coordinate system works.\n",
    "\n",
    "Let's validate that. We have a point in the world, we can attempt to pick-up from there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa1ecad05ba6b16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T11:29:44.122590Z",
     "start_time": "2025-03-04T11:29:30.537362Z"
    }
   },
   "outputs": [],
   "source": [
    "camera_rec.start_recording(file_path=\"pick_up/vid\")\n",
    "\n",
    "try:\n",
    "    pickup_point = point_world.copy()\n",
    "    # we detected the face of the cube. We want to grasp it slightly lower than that:\n",
    "    pickup_point[2] -= 0.01\n",
    "\n",
    "    # offset is only positive in Z direction, which means pick up from above:\n",
    "    robot.pick_up_at_angle(pickup_point, [0, 0, 0.1])\n",
    "    robot.put_down_at_angle([0.5, -0.2, 0.03], [0, 0, 0.1])\n",
    "finally:\n",
    "    camera_rec.stop_recording()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56a2ffe916e1e1d",
   "metadata": {},
   "source": [
    " ur_lab/vision/image_block_position_estimation.py is extended version of what we did here with some extra features. You may use it as reference for a slightly more complex version."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
